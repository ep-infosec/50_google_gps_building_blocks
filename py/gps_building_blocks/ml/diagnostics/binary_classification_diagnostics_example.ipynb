{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e290d603",
      "metadata": {
        "id": "e290d603"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bae240f",
      "metadata": {
        "id": "6bae240f"
      },
      "source": [
        "# Binary Classification Model Diagnostics Example Notebook\n",
        "\n",
        "This notebook demonstrates the uset of [Binary Classication Diagnostics](https://github.com/google/gps_building_blocks/blob/master/py/gps_building_blocks/ml/diagnostics/binary_classification.py) module."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZlF0N4BGsO52",
      "metadata": {
        "id": "ZlF0N4BGsO52"
      },
      "source": [
        "## Requirements:\n",
        "\n",
        "A test ML dataset (scored by a binary classification model) containing the original binary label, predicted probabilities (for the positive class) and features     is avilable\n",
        "as a csv file or a GCP BigQuery table.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aec16f3",
      "metadata": {
        "id": "1aec16f3"
      },
      "source": [
        "## Install and import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2212e2ae",
      "metadata": {
        "id": "2212e2ae"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install gps_building_blocks\n",
        "# !pip install gps_building_blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b4ca36",
      "metadata": {
        "id": "51b4ca36"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gps_building_blocks.cloud.utils import bigquery as bigquery_utils\n",
        "from gps_building_blocks.ml.diagnostics import binary_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17cb7cf1",
      "metadata": {
        "id": "17cb7cf1"
      },
      "source": [
        "## Set paramaters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01dce4e5",
      "metadata": {
        "id": "01dce4e5"
      },
      "outputs": [],
      "source": [
        "# If the scored test dataset is available as a GCP BigQuery table\n",
        "\n",
        "# GCP Project ID\n",
        "PROJECT_ID = 'project-id'\n",
        "# BigQuery dataset name\n",
        "DATASET = 'dataset'\n",
        "# BigQuery table name containing the scored test dataset\n",
        "TEST_DATA_PREDICTIONS_TABLE = 'test_prediction_table'\n",
        "\n",
        "bq_utils = bigquery_utils.BigQueryUtils(project_id=PROJECT_ID)\n",
        "\n",
        "# SQL for extracting prediction dataset when using BQML.\n",
        "sql = f\"\"\"\n",
        "  SELECT *\n",
        "  FROM `{PROJECT_ID}.{DATASET}.{TEST_DATA_PREDICTIONS_TABLE}`;\n",
        "\"\"\"\n",
        "print (sql)\n",
        "df_prediction = bq_utils.run_query(sql).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-sdxpPFs4ePH",
      "metadata": {
        "id": "-sdxpPFs4ePH"
      },
      "outputs": [],
      "source": [
        "# If the scored test dataset is available as a csv file\n",
        "\n",
        "# Csv file name of the scored test dataset \n",
        "TEST_DATA_PREDICTIONS_FILE = 'test_predictions.csv'\n",
        "\n",
        "df_prediction = pd.read_csv('TEST_DATA_PREDICTIONS_FILE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vdffk3705f4s",
      "metadata": {
        "id": "vdffk3705f4s"
      },
      "outputs": [],
      "source": [
        "# Print the dataset\n",
        "df_prediction.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ogv1VkhF5mSD",
      "metadata": {
        "id": "ogv1VkhF5mSD"
      },
      "outputs": [],
      "source": [
        "# Name of the column in the prediction table with the predicted probabilities\n",
        "PREDICTED_LABEL_NAME = 'predicted_label_probs'\n",
        "# Name of the column in the prediction table with the actual label\n",
        "ACTUAL_LABEL_NAME = 'label'\n",
        "# Label value for the positive class\n",
        "POSITIVE_CLASS_LABEL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a09646a",
      "metadata": {
        "id": "5a09646a"
      },
      "source": [
        "## Transform the label values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e98876dd",
      "metadata": {
        "id": "e98876dd"
      },
      "outputs": [],
      "source": [
        "# Check the actual label values\n",
        "df_prediction[ACTUAL_LABEL_NAME].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f35dee5",
      "metadata": {
        "id": "4f35dee5"
      },
      "outputs": [],
      "source": [
        "# Convert the actual label values into 1.0 and 0.0 if the they are different\n",
        "df_prediction['label_numerical'] = [1.0 if label == POSITIVE_CLASS_LABEL\n",
        "                                    else 0.0 for label in \n",
        "                                    df_prediction[ACTUAL_LABEL_NAME]]\n",
        "\n",
        "# Check the transformed label values\n",
        "df_prediction['label_numerical'].value_counts() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qP1ihsgg7ZgS",
      "metadata": {
        "id": "qP1ihsgg7ZgS"
      },
      "source": [
        "## Run model diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31329453",
      "metadata": {
        "id": "31329453"
      },
      "source": [
        "## Performance metrics\n",
        "\n",
        "The following function calculates below metrics:\n",
        "\n",
        "* `prop_positives`: Proportion of positive instances in the dataset scored\n",
        "* `auc_roc`: Area under the ROC curve (more on this below).\n",
        "* `auc_pr`: Area under the Precision Recall Curve (more on this below).\n",
        "* `binarize_threshold`: The probability threshold used to binarize the predictions to calculate the following performance metrics.\n",
        "* `accuracy`: overall accuracy of the predictions.\n",
        "* `true_positive_rate`: (Recall or Sensitivity) proportion of positive. instances correctly predicted out of all the positive instances in the dataset.\n",
        "* `true_negative_rate`: (Specificity) proportion of negative instances. correctly predicted out of all the negative instances in the dataset.\n",
        "* `precision`: (Confidence) proportion of positive instances correctly predicted out of all the instances predicted as positives.\n",
        "* `f1_score`: A weighted average between Precision and Recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1d7182",
      "metadata": {
        "id": "7b1d7182"
      },
      "outputs": [],
      "source": [
        "binary_classification.calc_performance_metrics(\n",
        "    labels = df_prediction['label_numerical'].values,\n",
        "    probability_predictions = df_prediction[PREDICTED_LABEL_NAME].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dbf1842",
      "metadata": {
        "id": "1dbf1842"
      },
      "source": [
        "## ROC curve (True Positive Rate vs False Positive Rate)\n",
        "\n",
        "The following function plots the receiver operating characteristic (ROC) curve, which illustrates the relationship between true positive rate and false positive rate of the predictions from a binary classifier system. As the major metric to evaluate the ROC curve, the AUC (Area Under The Curve) is also printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf1d25e",
      "metadata": {
        "id": "bbf1d25e"
      },
      "outputs": [],
      "source": [
        "binary_classification.plot_roc_curve(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction[PREDICTED_LABEL_NAME].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe91cb4f",
      "metadata": {
        "id": "fe91cb4f"
      },
      "source": [
        "## Precision-Recall curve\n",
        "\n",
        "The following function plots the precision-recall curve, which illustrates the relationship between the precision and recall of the predictions from a binary classifier system. In general a high precision rate relates to a low false positive rate, while a high recall rate relates to a low false negative rate. By showing the tradeoff between precision and recall with different thresholds on the predictions, we are able to find out the optimal threshold for the specific problem. Moreover, the Average Precision (AP), which calculates the area under the precision-recall curve, is also printed for the evaluation of precision recall curve.[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b82fbe5c",
      "metadata": {
        "id": "b82fbe5c"
      },
      "outputs": [],
      "source": [
        "binary_classification.plot_precision_recall_curve(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction[PREDICTED_LABEL_NAME].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538d41e9",
      "metadata": {
        "id": "538d41e9"
      },
      "source": [
        "## Predicted probability distributions\n",
        "\n",
        "The following function plots the distributions of predicted probabilities for each class. This illustrates how distinguishable the predicted probabilities for different classes are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a478d2bf",
      "metadata": {
        "id": "a478d2bf"
      },
      "outputs": [],
      "source": [
        "binary_classification.plot_predicted_probabilities(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction[PREDICTED_LABEL_NAME].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7083c98e",
      "metadata": {
        "id": "7083c98e"
      },
      "source": [
        "## Calculate and plot performance metrics for Bins of the Probabilities\n",
        "\n",
        "The following function does following:\n",
        "\n",
        "1. Sorts predicted probabilities in the descending order, then divides the instances into N number of equal sized bins (e.g. deciles when N = 10) such that the first bin has the instances with the highest probabilities and so on.\n",
        "2. Calculates the Precision, Precision Uplift (Precision of the bin divided by the proportion of positive instances in the dataset indicating the Precision of selecting a random sample of the size of the bin) and Coverage (or Recall = the proportion of positive instances in the bin out of all the positive instances in the dataset) for each bin.\n",
        "\n",
        "**Observations:** These plots can give a good understanding of the model performance for these different bins of the probability predictions. Generally we would expect to see a monotonically decreasing trend of these metrics going from the top bin to the bottom bin. If we see some different pattern from this plot, it would create some doubts of the quality of the model leading to further investigations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8678007",
      "metadata": {
        "id": "a8678007"
      },
      "outputs": [],
      "source": [
        "bin_metrics = binary_classification.calc_bin_metrics(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction[PREDICTED_LABEL_NAME].values,\n",
        "    number_bins=10,\n",
        "    decimal_points=4)\n",
        "\n",
        "binary_classification.plot_bin_metrics(bin_metrics=bin_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b73d0f",
      "metadata": {
        "id": "30b73d0f"
      },
      "outputs": [],
      "source": [
        "bin_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833746d8",
      "metadata": {
        "id": "833746d8"
      },
      "source": [
        "## Calculate and plot performance metrics for Cumulative Bins of the Probabilities\n",
        "\n",
        "The following function plots does following:\n",
        "\n",
        "1. Sorts predicted probabilities in the descending order, and then divides the instances into N number of bins with increasing size. For example, when N = 10, it creates 10 bins such that the first bin contains the top 10% instances with the highest probability, the second bin contains the top 20% instances with the highest probability and so on where the last bin contains all the instances.\n",
        "2. Calculates the Precision, Precision Uplift (Precision of the bin divided by the proportion of positive instances in the dataset indicating the Precision of selecting a random sample of the size of the bin) and Coverage (or Recall = the proportion of positive instances in the bin out of all the positive instances in the dataset) for each bin.\n",
        "\n",
        "**Observations:** From these plots we would generally expect the Precision and Precision uplift to monotonically decrease and Recall to monotonically increase when we go from the high probability bins to the lower ones. These plots give us a good understanding of the expected precision and recall values for example if we select top N% of the instances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5451547",
      "metadata": {
        "id": "e5451547"
      },
      "outputs": [],
      "source": [
        "cumulative_bin_metrics = binary_classification.calc_cumulative_bin_metrics(\n",
        "    labels=df_prediction['label_numerical'].values,\n",
        "    probability_predictions=df_prediction[PREDICTED_LABEL_NAME].values,\n",
        "    number_bins=10,\n",
        "    decimal_points=4)\n",
        "\n",
        "binary_classification.plot_cumulative_bin_metrics(\n",
        "    cumulative_bin_metrics=cumulative_bin_metrics);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d76de6",
      "metadata": {
        "id": "97d76de6"
      },
      "source": [
        "## Feature exploration plots\n",
        "\n",
        "This section calculates and plots the distribution of features in the dataset for the equal sized bins of the predicted probability.\n",
        "\n",
        "How it works:\n",
        "1. First sorts predicted probabilities in the descending order, and then divides the instances into N number of equal sized bins (e.g. deciles when N = 10) such that the first bin has the instances with the highest probabilities and so on.\n",
        "2. Then it calculates and plots the distribution of each feature for each plot.\n",
        "\n",
        "**Expected outputs:**\n",
        "\n",
        "The output plots can be used to understand the relationships (positive, negative and non-linear correlations) between the features and the predictions (labels) leading to:\n",
        "\n",
        "* extracting new insights in relation to the business problem (e.g. how the demographics, user-behaviour or functionalities in the app/website related to the conversion rate).\n",
        "* confirming the relationships the model has learned between features and label makes sense (e.g. it hasn't learned any suspicious relationships caused by label leakages due to data collection or processing issues).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f19479",
      "metadata": {
        "id": "52f19479"
      },
      "outputs": [],
      "source": [
        "# If the scored test dataset is available as a GCP BigQuery table we can read in \n",
        "# the table schema to identify data types of the features\n",
        "\n",
        "sql = f\"\"\"SELECT column_name, data_type\n",
        "       FROM `{PROJECT_ID}.{DATASET}`.INFORMATION_SCHEMA.COLUMNS\n",
        "       WHERE table_name='{TEST_DATA_PREDICTIONS_TABLE}'\"\"\"\n",
        "\n",
        "print(sql)\n",
        "schema = bq_utils.run_query(sql).to_dataframe()\n",
        "schema.columns = ['column_name', 'type']\n",
        "print(schema.head())\n",
        "\n",
        "# Seperate out numerical and categorical columns names.\n",
        "num_features = list(schema[schema['type'].isin(['INT64','FLOAT64'])]['column_name'])\n",
        "cat_features = list(schema[schema['type'].isin(['STRING'])]['column_name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb01a81a",
      "metadata": {
        "id": "fb01a81a"
      },
      "outputs": [],
      "source": [
        "# If the scored test dataset is available as a csv file   \n",
        "\n",
        "# List of numerical features to plot\n",
        "num_features = [...]\n",
        "# List of categorical features to plot\n",
        "cat_features = [...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb948aa7",
      "metadata": {
        "id": "eb948aa7"
      },
      "outputs": [],
      "source": [
        "# Remove non-feature columns.\n",
        "columns_to_remove = ['user_id', 'label']\n",
        "num_features = [v for v in num_features if v not in columns_to_remove]\n",
        "cat_features = [v for v in cat_features if v not in columns_to_remove]\n",
        "print(f'Number of numerical features: {len(num_features)}')\n",
        "print(f'Number of categorical features: {len(cat_features)}')\n",
        "\n",
        "feature_names_to_plot = num_features + cat_features\n",
        "feature_types = (['numerical'] * len(num_features) + \n",
        "                 ['categorical'] * len(cat_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14becc7c",
      "metadata": {
        "id": "14becc7c"
      },
      "outputs": [],
      "source": [
        "binary_classification.plot_binned_features(\n",
        "    data=df_prediction,\n",
        "    prediction_column_name=PREDICTED_LABEL_NAME,\n",
        "    feature_names=feature_names_to_plot,\n",
        "    feature_types=feature_types);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "binary_classification_diagnostics_example",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/05.model_evaluation_and_diagnostics.ipynb?workspaceId=szczecinski:dev_model_diagnostics::citc",
          "timestamp": 1631805049021
        },
        {
          "file_id": "1MgZHVijNLCNIXbwU-zKPrXHl4sJZ8RHT",
          "timestamp": 1631262181717
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/5_model_evaluation_and_diagnostics.ipynb?workspaceId=szczecinski:diagnostic_prod::citc",
          "timestamp": 1626444813368
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/compass/packages/propensity/5_model_evaluation_and_diagnostics.ipynb?workspaceId=szczecinski:diagnostic_prod::citc",
          "timestamp": 1626442594736
        },
        {
          "file_id": "1dugCt0TXinm1NqCEUiuFfyxWS54mDekp",
          "timestamp": 1625469937832
        }
      ]
    },
    "environment": {
      "name": "common-cpu.m78",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m78"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
